{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    cohen_kappa_score,\n",
    "    matthews_corrcoef\n",
    ")\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.base import clone\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  studies_series: 2974\n",
      "  X_df: (60660, 2974)\n",
      "  y_series: 2974\n",
      "  Studies: 2974\n",
      "  X shape: (2974, 60660)\n",
      "  y: 2974\n",
      "  Studies: 1914\n",
      "  X shape: (1914, 60660)\n",
      "  y: 1914\n"
     ]
    }
   ],
   "source": [
    "from train_test import load_data, filter_data, encode_labels\n",
    "X, y, study_labels = load_data(\"/home/jeppe/Documents/Leukem.ai/data\")\n",
    "X, y, study_labels = filter_data(X, y, study_labels, min_n = 20)\n",
    "y, label_mapping = encode_labels(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedXGBClassifier:\n",
    "    def __init__(self, class_weight=False, **xgb_params):\n",
    "        self.class_weight = class_weight\n",
    "        self.xgb_params = xgb_params\n",
    "        self.model = XGBClassifier(**xgb_params)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if self.class_weight:\n",
    "            sample_weights = class_weight.compute_sample_weight(class_weight='balanced', y=y)\n",
    "        else:\n",
    "            sample_weights = None\n",
    "\n",
    "        self.model.fit(X, y, sample_weight=sample_weights)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self.model.predict_proba(X)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        return self.model.score(X, y)\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        # Include class_weight in params for grid search\n",
    "        return {'class_weight': self.class_weight, **self.model.get_params(deep)}\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        # Extract and store class_weight separately\n",
    "        if 'class_weight' in params:\n",
    "            self.class_weight = params.pop('class_weight')\n",
    "        \n",
    "        self.model.set_params(**params)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV setup\n",
    "outer_cv = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n",
    "inner_cv = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_genes': [1000, 2000, 3000],\n",
    "    'C': [10, 100, 1000],\n",
    "    'gamma': ['auto', 0.001, 0.01],\n",
    "    'kernel': ['rbf'],\n",
    "    'class_weight': [\"balanced\", None],\n",
    "    'probability': [True]\n",
    "}\n",
    "param_grid = {\n",
    "    'n_genes': [1000],\n",
    "    'class_weight': [True, False],\n",
    "    'max_depth': [2,5]\n",
    "}\n",
    "param_combos = list(ParameterGrid(param_grid))\n",
    "\n",
    "#model = SVC\n",
    "model = WeightedXGBClassifier\n",
    "pipe = Pipeline([\n",
    "    ('DEseq2', transformers.DESeq2RatioNormalizer()),\n",
    "    ('feature_selection', transformers.FeatureSelection()),\n",
    "    ('scaler', StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeppe/Documents/Leukem.ai/.venv/lib/python3.12/site-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_fold\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeppe/Documents/Leukem.ai/.venv/lib/python3.12/site-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner_fold\n",
      "0\n",
      "inner_fold\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done   3 out of   8 | elapsed:   27.3s remaining:   45.5s\n",
      "[Parallel(n_jobs=12)]: Done   8 out of   8 | elapsed:   31.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_fold\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeppe/Documents/Leukem.ai/.venv/lib/python3.12/site-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner_fold\n",
      "0\n",
      "inner_fold\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done   3 out of   8 | elapsed:   19.2s remaining:   32.0s\n",
      "[Parallel(n_jobs=12)]: Done   8 out of   8 | elapsed:   23.5s finished\n"
     ]
    }
   ],
   "source": [
    "studies_as_folds = [\n",
    "        \"BEATAML1.0-COHORT\",\n",
    "        \"AAML0531\",\n",
    "        \"AAML1031\",\n",
    "        \"TCGA-LAML\",\n",
    "        \"LEUCEGENE\"\n",
    "    ]\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Function to evaluate one inner fold + hyperparam combo\n",
    "def evaluate_inner_fold(outer_fold, inner_fold,\n",
    "                        processed_X, y_train_inner, y_val_inner,\n",
    "                        model,\n",
    "                        params,\n",
    "                        type = \"standard\"):\n",
    "    \n",
    "      \n",
    "    def standard_eval():\n",
    "        le = LabelEncoder()\n",
    "        y_train_inner_enc = le.fit_transform(y_train_inner)\n",
    "        clf.fit(X_train_inner, y_train_inner_enc)\n",
    "        preds = clf.predict(X_val_inner)\n",
    "        preds = le.inverse_transform(preds)\n",
    "        \n",
    "        return {\n",
    "            'outer_fold': outer_fold,\n",
    "            'inner_fold': inner_fold,\n",
    "            'params': params,\n",
    "            'accuracy': accuracy_score(y_val_inner, preds),\n",
    "            'f1_macro': f1_score(y_val_inner, preds, average='macro'),\n",
    "            'mcc': matthews_corrcoef(y_val_inner, preds),\n",
    "            'kappa': cohen_kappa_score(y_val_inner, preds)\n",
    "        }\n",
    "\n",
    "    def ovr_eval():\n",
    "        results = []\n",
    "        classes = np.unique(y_train_inner)\n",
    "        for cl in classes:\n",
    "            y_train_bin = [1 if yy == cl else 0 for yy in y_train_inner]\n",
    "            y_val_bin = [1 if yy == cl else 0 for yy in y_val_inner]\n",
    "\n",
    "            y_train_bin = np.array(y_train_bin, dtype=np.int32)\n",
    "            y_val_bin = np.array(y_val_bin, dtype=np.int32)\n",
    "\n",
    "            clf.fit(X_train_inner, y_train_bin)\n",
    "            preds = clf.predict_proba(X_val_inner)\n",
    "            preds = preds[:, 1]\n",
    "            preds = (preds >= 0.5).astype(int)\n",
    "            results.append({\n",
    "                'outer_fold': outer_fold,\n",
    "                'inner_fold': inner_fold,\n",
    "                'class': cl,\n",
    "                'params': params,\n",
    "                'accuracy': accuracy_score(y_val_bin, preds),\n",
    "                'f1_binary': f1_score(y_val_bin, preds, average='binary', pos_label=1),\n",
    "                'mcc': matthews_corrcoef(y_val_bin, preds),\n",
    "                'kappa': cohen_kappa_score(y_val_bin, preds)\n",
    "            })\n",
    "        return results\n",
    "\n",
    "    def ovo_eval():\n",
    "        results = []\n",
    "        classes = np.unique(y_train_inner)\n",
    "        for i, j in itertools.combinations(classes, 2):\n",
    "            train_mask = [(yy == i or yy == j) for yy in y_train_inner]\n",
    "            val_mask = [(yy == i or yy == j) for yy in y_val_inner]\n",
    "\n",
    "            X_train_ij = X_train_inner[train_mask]\n",
    "            y_train_ij = np.array([yy for yy in y_train_inner if yy == i or yy == j], dtype=np.int32) \n",
    "            y_train_ij = (y_train_ij == i).astype(np.int32)\n",
    "             \n",
    "            X_val_ij = X_val_inner[val_mask]\n",
    "            y_val_ij = np.array([yy for yy in y_val_inner if yy == i or yy == j], dtype=np.int32)\n",
    "            y_val_ij = (y_val_ij == i).astype(np.int32) \n",
    "\n",
    "            clf.fit(X_train_ij, y_train_ij)\n",
    "            preds = clf.predict_proba(X_val_ij)\n",
    "            preds = preds[:, 1]\n",
    "            preds = (preds >= 0.5).astype(int)\n",
    "            results.append({\n",
    "                'outer_fold': outer_fold,\n",
    "                'inner_fold': inner_fold,\n",
    "                'class_0': i,\n",
    "                'class_1': j,\n",
    "                'params': params,\n",
    "                'accuracy': accuracy_score(y_val_ij, preds),\n",
    "                'f1_binary': f1_score(y_val_ij, preds, average='binary', pos_label=1),\n",
    "                'mcc': matthews_corrcoef(y_val_ij, preds),\n",
    "                'kappa': cohen_kappa_score(y_val_ij, preds)\n",
    "            })\n",
    "        return results\n",
    "\n",
    "    # Dispatch table for clean logic\n",
    "    eval_dispatch = {\n",
    "        'standard': standard_eval,\n",
    "        'OvR': ovr_eval,\n",
    "        'OvO': ovo_eval\n",
    "    }\n",
    "\n",
    "    if type not in eval_dispatch:\n",
    "        raise ValueError(f\"Unsupported evaluation type: {type}\")\n",
    "    \n",
    "    # Select preprocessed data\n",
    "    n_genes = params.pop('n_genes')\n",
    "    X_train_inner, X_val_inner = processed_X[n_genes]\n",
    "\n",
    "    # Set classifier\n",
    "    clf = clone(model(**params))\n",
    "    params['n_genes'] = n_genes\n",
    "\n",
    "    return eval_dispatch[type]()\n",
    "\n",
    "def pre_process_data(n_genes_list, X_train_outer, train_inner_idx, val_inner_idx, study_labels_outer, pipe):\n",
    "        \n",
    "        X_train_inner = X_train_outer[train_inner_idx]\n",
    "        X_val_inner = X_train_outer[val_inner_idx]\n",
    "\n",
    "        study_labels_inner = study_labels_outer[train_inner_idx]\n",
    "        \n",
    "        y_train_inner = y_train_outer[train_inner_idx]\n",
    "        y_val_inner = y_train_outer[val_inner_idx]\n",
    "\n",
    "        y_train_inner = np.array(y_train_inner, dtype=np.int32)\n",
    "        y_val_inner = np.array(y_val_inner, dtype=np.int32)\n",
    "        \n",
    "        processed_X = {}\n",
    "        for n_genes_i in n_genes_list:\n",
    "            pipe_inner = clone(pipe)\n",
    "\n",
    "            X_train_inner_proc = pipe_inner.fit_transform(X_train_inner, \n",
    "                                                feature_selection__study_per_patient=study_labels_inner, \n",
    "                                                feature_selection__n_genes=n_genes_i)\n",
    "            X_val_inner_proc = pipe_inner.transform(X_val_inner)\n",
    "\n",
    "\n",
    "            processed_X[n_genes_i] = [X_train_inner_proc, X_val_inner_proc]\n",
    "        return processed_X, y_train_inner, y_val_inner\n",
    "\n",
    "all_results = []\n",
    "\n",
    "combined = [str(a) + \" \" + str(b) for a, b in zip(y, study_labels)]\n",
    "\n",
    "for outer_fold, (train_idx, test_idx) in enumerate(outer_cv.split(X, combined)):\n",
    "    print(\"outer_fold\")\n",
    "    print(outer_fold)\n",
    "    X_train_outer = X[train_idx]\n",
    "    y_train_outer = y[train_idx]\n",
    "    study_labels_outer = study_labels[train_idx]\n",
    "    \n",
    "    combined_outer = [str(a) + \" \" + str(b) for a, b in zip(y_train_outer, study_labels_outer)]\n",
    "    \n",
    "    inner_tasks = []\n",
    "    for inner_fold, (train_inner_idx, val_inner_idx) in enumerate(inner_cv.split(X_train_outer, combined_outer)):\n",
    "        print(\"inner_fold\")\n",
    "        print(inner_fold)\n",
    "\n",
    "        processed_X, y_train_inner, y_val_inner = pre_process_data(\n",
    "            param_grid[\"n_genes\"], \n",
    "            X_train_outer, \n",
    "            train_inner_idx, \n",
    "            val_inner_idx, \n",
    "            study_labels_outer,\n",
    "            pipe)\n",
    "\n",
    "        for params in param_combos:\n",
    "            inner_tasks.append(delayed(evaluate_inner_fold)(\n",
    "                outer_fold, inner_fold,\n",
    "                processed_X, y_train_inner, y_val_inner,\n",
    "                model,\n",
    "                params,\n",
    "                type = \"standard\" # standard, OvR, OvO\n",
    "            ))\n",
    "\n",
    "    # Run inner CV tasks in parallel (adjust n_jobs to number of CPU cores)\n",
    "    inner_results = Parallel(n_jobs=12, verbose=1)(inner_tasks)\n",
    "    if isinstance(inner_results[0], dict):\n",
    "        # Flat list of dictionaries\n",
    "        all_results.extend(inner_results)\n",
    "    elif isinstance(inner_results[0], list):\n",
    "        # List of lists of dictionaries\n",
    "        for res in inner_results:\n",
    "            all_results.extend(res)\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected structure in inner_results\")\n",
    "\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_parallel_results = pd.DataFrame(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.7161821410455829)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_cv_results(df, param_grid, label_mapping, score_col):\n",
    "    #  Extract param names and expand 'params'\n",
    "    param_names = list(param_grid.keys())\n",
    "    params_df = df['params'].apply(pd.Series)\n",
    "\n",
    "    # Normalize None values for groupby\n",
    "    for col in param_names:\n",
    "        if col in params_df.columns:\n",
    "            params_df[col] = params_df[col].apply(lambda x: 'none' if x is None else x)\n",
    "\n",
    "    #Combine expanded params with original DataFrame\n",
    "    df_with_params = pd.concat([df.drop(columns=['params']), params_df], axis=1)\n",
    "\n",
    "    # Determine group-by strategy based on evaluation type\n",
    "    if 'class' in df_with_params.columns:\n",
    "        # OvR\n",
    "        group_cols = param_names + ['class']\n",
    "        summary = df_with_params.groupby(group_cols)[score_col].mean().reset_index()\n",
    "        best = summary.loc[summary.groupby('class')[score_col].idxmax()].reset_index(drop=True)\n",
    "\n",
    "    elif 'class_0' in df_with_params.columns and 'class_1' in df_with_params.columns:\n",
    "        # OvO\n",
    "        group_cols = param_names + ['class_0', 'class_1']\n",
    "        summary = df_with_params.groupby(group_cols)[score_col].mean().reset_index()\n",
    "        best = summary.loc[summary.groupby(['class_0', 'class_1'])[score_col].idxmax()].reset_index(drop=True)\n",
    "\n",
    "    else:\n",
    "        # Standard multiclass\n",
    "        group_cols = param_names\n",
    "        summary = df_with_params.groupby(group_cols)[score_col].mean().reset_index()\n",
    "        best = summary.loc[[summary[score_col].idxmax()]].reset_index(drop=True)\n",
    "\n",
    "    int_to_label = {v: k for k, v in label_mapping.items()}\n",
    "    if 'class' in best.columns:\n",
    "        # OvR case\n",
    "        best['class_label'] = best['class'].map(int_to_label)\n",
    "        return best\n",
    "\n",
    "    elif 'class_0' in best.columns and 'class_1' in best.columns:\n",
    "        # OvO case\n",
    "        best['class_0_label'] = best['class_0'].map(int_to_label)\n",
    "        best['class_1_label'] = best['class_1'].map(int_to_label)\n",
    "        return best\n",
    "\n",
    "    else:\n",
    "        return best\n",
    "\n",
    "best_per_class_df = process_cv_results(\n",
    "    df_parallel_results,\n",
    "    param_grid=param_grid,\n",
    "    label_mapping = label_mapping,\n",
    "    score_col = \"kappa\"\n",
    ")\n",
    "best_per_class_df[\"kappa\"].mean() # np.float64(0.5714442426161126)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Outer Loop: Holding out Study 'BEATAML1.0-COHORT' for Testing ---\n",
      "Outer training set contains studies: ['AAML0531', 'AAML1031', 'LEUCEGENE', 'TCGA-LAML']\n",
      "  Inner Loop: Validating on Study 'AAML0531'\n",
      "    Preprocessing data for inner fold (Train N=1042, Val N=486)...\n",
      "  For study: BEATAML1.0-COHORT, mask sum == 0\n",
      "  For study: AAML0531, mask sum == 0\n",
      "    Preprocessing done.\n",
      "  Inner Loop: Validating on Study 'AAML1031'\n",
      "    Preprocessing data for inner fold (Train N=984, Val N=544)...\n",
      "  For study: BEATAML1.0-COHORT, mask sum == 0\n",
      "  For study: AAML1031, mask sum == 0\n",
      "    Preprocessing done.\n",
      "  Inner Loop: Validating on Study 'LEUCEGENE'\n",
      "    Preprocessing data for inner fold (Train N=1159, Val N=369)...\n",
      "  For study: BEATAML1.0-COHORT, mask sum == 0\n",
      "  For study: LEUCEGENE, mask sum == 0\n",
      "    Preprocessing done.\n",
      "  Inner Loop: Validating on Study 'TCGA-LAML'\n",
      "    Preprocessing data for inner fold (Train N=1399, Val N=129)...\n",
      "  For study: BEATAML1.0-COHORT, mask sum == 0\n",
      "  For study: TCGA-LAML, mask sum == 0\n",
      "    Preprocessing done.\n",
      "  Executing 16 inner evaluation tasks in parallel (n_jobs=12)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  10 out of  16 | elapsed:  2.9min remaining:  1.7min\n",
      "[Parallel(n_jobs=12)]: Done  16 out of  16 | elapsed:  3.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Finished evaluations for outer fold 'BEATAML1.0-COHORT'.\n",
      "\n",
      "--- Outer Loop: Holding out Study 'AAML0531' for Testing ---\n",
      "Outer training set contains studies: ['AAML1031', 'BEATAML1.0-COHORT', 'LEUCEGENE', 'TCGA-LAML']\n",
      "  Inner Loop: Validating on Study 'AAML1031'\n",
      "    Preprocessing data for inner fold (Train N=884, Val N=544)...\n",
      "  For study: AAML0531, mask sum == 0\n",
      "  For study: AAML1031, mask sum == 0\n",
      "    Preprocessing done.\n",
      "  Inner Loop: Validating on Study 'BEATAML1.0-COHORT'\n",
      "    Preprocessing data for inner fold (Train N=1042, Val N=386)...\n",
      "  For study: BEATAML1.0-COHORT, mask sum == 0\n",
      "  For study: AAML0531, mask sum == 0\n",
      "    Preprocessing done.\n",
      "  Inner Loop: Validating on Study 'LEUCEGENE'\n",
      "    Preprocessing data for inner fold (Train N=1059, Val N=369)...\n",
      "  For study: AAML0531, mask sum == 0\n",
      "  For study: LEUCEGENE, mask sum == 0\n",
      "    Preprocessing done.\n",
      "  Inner Loop: Validating on Study 'TCGA-LAML'\n",
      "    Preprocessing data for inner fold (Train N=1299, Val N=129)...\n",
      "  For study: AAML0531, mask sum == 0\n",
      "  For study: TCGA-LAML, mask sum == 0\n",
      "    Preprocessing done.\n",
      "  Executing 16 inner evaluation tasks in parallel (n_jobs=12)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  10 out of  16 | elapsed:   38.6s remaining:   23.1s\n",
      "[Parallel(n_jobs=12)]: Done  16 out of  16 | elapsed:   50.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Finished evaluations for outer fold 'AAML0531'.\n",
      "\n",
      "--- Outer Loop: Holding out Study 'AAML1031' for Testing ---\n",
      "Outer training set contains studies: ['AAML0531', 'BEATAML1.0-COHORT', 'LEUCEGENE', 'TCGA-LAML']\n",
      "  Inner Loop: Validating on Study 'AAML0531'\n",
      "    Preprocessing data for inner fold (Train N=884, Val N=486)...\n",
      "  For study: AAML0531, mask sum == 0\n",
      "  For study: AAML1031, mask sum == 0\n",
      "    Preprocessing done.\n",
      "  Inner Loop: Validating on Study 'BEATAML1.0-COHORT'\n",
      "    Preprocessing data for inner fold (Train N=984, Val N=386)...\n",
      "  For study: BEATAML1.0-COHORT, mask sum == 0\n",
      "  For study: AAML1031, mask sum == 0\n",
      "    Preprocessing done.\n",
      "  Inner Loop: Validating on Study 'LEUCEGENE'\n",
      "    Preprocessing data for inner fold (Train N=1001, Val N=369)...\n",
      "  For study: AAML1031, mask sum == 0\n",
      "  For study: LEUCEGENE, mask sum == 0\n",
      "    Preprocessing done.\n",
      "  Inner Loop: Validating on Study 'TCGA-LAML'\n",
      "    Preprocessing data for inner fold (Train N=1241, Val N=129)...\n",
      "  For study: AAML1031, mask sum == 0\n",
      "  For study: TCGA-LAML, mask sum == 0\n",
      "    Preprocessing done.\n",
      "  Executing 16 inner evaluation tasks in parallel (n_jobs=12)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  10 out of  16 | elapsed:   38.0s remaining:   22.8s\n",
      "[Parallel(n_jobs=12)]: Done  16 out of  16 | elapsed:   49.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Finished evaluations for outer fold 'AAML1031'.\n",
      "\n",
      "--- Outer Loop: Holding out Study 'TCGA-LAML' for Testing ---\n",
      "Outer training set contains studies: ['AAML0531', 'AAML1031', 'BEATAML1.0-COHORT', 'LEUCEGENE']\n",
      "  Inner Loop: Validating on Study 'AAML0531'\n",
      "    Preprocessing data for inner fold (Train N=1299, Val N=486)...\n",
      "  For study: AAML0531, mask sum == 0\n",
      "  For study: TCGA-LAML, mask sum == 0\n",
      "    Preprocessing done.\n",
      "  Inner Loop: Validating on Study 'AAML1031'\n",
      "    Preprocessing data for inner fold (Train N=1241, Val N=544)...\n",
      "  For study: AAML1031, mask sum == 0\n",
      "  For study: TCGA-LAML, mask sum == 0\n",
      "    Preprocessing done.\n",
      "  Inner Loop: Validating on Study 'BEATAML1.0-COHORT'\n",
      "    Preprocessing data for inner fold (Train N=1399, Val N=386)...\n",
      "  For study: BEATAML1.0-COHORT, mask sum == 0\n",
      "  For study: TCGA-LAML, mask sum == 0\n",
      "    Preprocessing done.\n",
      "  Inner Loop: Validating on Study 'LEUCEGENE'\n",
      "    Preprocessing data for inner fold (Train N=1416, Val N=369)...\n",
      "  For study: TCGA-LAML, mask sum == 0\n",
      "  For study: LEUCEGENE, mask sum == 0\n",
      "    Preprocessing done.\n",
      "  Executing 16 inner evaluation tasks in parallel (n_jobs=12)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  10 out of  16 | elapsed:   56.7s remaining:   34.0s\n",
      "[Parallel(n_jobs=12)]: Done  16 out of  16 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Finished evaluations for outer fold 'TCGA-LAML'.\n",
      "\n",
      "--- Outer Loop: Holding out Study 'LEUCEGENE' for Testing ---\n",
      "Outer training set contains studies: ['AAML0531', 'AAML1031', 'BEATAML1.0-COHORT', 'TCGA-LAML']\n",
      "  Inner Loop: Validating on Study 'AAML0531'\n",
      "    Preprocessing data for inner fold (Train N=1059, Val N=486)...\n",
      "  For study: AAML0531, mask sum == 0\n",
      "  For study: LEUCEGENE, mask sum == 0\n",
      "    Preprocessing done.\n",
      "  Inner Loop: Validating on Study 'AAML1031'\n",
      "    Preprocessing data for inner fold (Train N=1001, Val N=544)...\n",
      "  For study: AAML1031, mask sum == 0\n",
      "  For study: LEUCEGENE, mask sum == 0\n",
      "    Preprocessing done.\n",
      "  Inner Loop: Validating on Study 'BEATAML1.0-COHORT'\n",
      "    Preprocessing data for inner fold (Train N=1159, Val N=386)...\n",
      "  For study: BEATAML1.0-COHORT, mask sum == 0\n",
      "  For study: LEUCEGENE, mask sum == 0\n",
      "    Preprocessing done.\n",
      "  Inner Loop: Validating on Study 'TCGA-LAML'\n",
      "    Preprocessing data for inner fold (Train N=1416, Val N=129)...\n",
      "  For study: TCGA-LAML, mask sum == 0\n",
      "  For study: LEUCEGENE, mask sum == 0\n",
      "    Preprocessing done.\n",
      "  Executing 16 inner evaluation tasks in parallel (n_jobs=12)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  10 out of  16 | elapsed:   38.8s remaining:   23.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Finished evaluations for outer fold 'LEUCEGENE'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  16 out of  16 | elapsed:   52.5s finished\n"
     ]
    }
   ],
   "source": [
    "# Define the studies to use as folds\n",
    "studies_as_folds = [\n",
    "    \"BEATAML1.0-COHORT\",\n",
    "    \"AAML0531\",\n",
    "    \"AAML1031\",\n",
    "    \"TCGA-LAML\",\n",
    "    \"LEUCEGENE\"\n",
    "]\n",
    "\n",
    "def pre_process_data_study_based(n_genes_list,\n",
    "                                 X_train_inner, X_val_inner,\n",
    "                                 y_train_inner, y_val_inner, # y values aren't strictly needed here but kept for consistency\n",
    "                                 study_labels_inner, # Labels corresponding to X_train_inner\n",
    "                                 pipe):\n",
    "    \"\"\"\n",
    "    Preprocesses inner training and validation sets for different n_genes.\n",
    "    Fits the pipeline ONLY on the inner training set.\n",
    "    \"\"\"\n",
    "    processed_X = {}\n",
    "    for n_genes_i in n_genes_list:\n",
    "        # Clone the pipeline for this specific n_genes setting\n",
    "        pipe_inner = clone(pipe)\n",
    "\n",
    "        X_train_inner_proc = pipe_inner.fit_transform(X_train_inner, y_train_inner, # Pass y if needed by steps\n",
    "                                                  feature_selection__study_per_patient=study_labels_inner,\n",
    "                                                  feature_selection__n_genes=n_genes_i)\n",
    "\n",
    "        # Transform the inner validation data (1 study) using the fitted pipeline\n",
    "        X_val_inner_proc = pipe_inner.transform(X_val_inner)\n",
    "\n",
    "        processed_X[n_genes_i] = [X_train_inner_proc, X_val_inner_proc]\n",
    "\n",
    "    # Return the dictionary of processed data and the original inner y values\n",
    "    return processed_X, y_train_inner, y_val_inner\n",
    "\n",
    "\n",
    "all_results = []\n",
    "n_jobs = 12 # Set desired number of parallel jobs\n",
    "X = np.array(X, dtype=np.float32)\n",
    "\n",
    "n_genes_list = param_grid[\"n_genes\"] # Get list of n_genes to process\n",
    "\n",
    "# Outer Loop: Iterate through each study to be used as the TEST set\n",
    "for test_study_name in studies_as_folds:\n",
    "    print(f\"\\n--- Outer Loop: Holding out Study '{test_study_name}' for Testing ---\")\n",
    "\n",
    "    # Create masks for outer split\n",
    "    test_mask = (study_labels == test_study_name)\n",
    "    train_mask = ~test_mask\n",
    "\n",
    "    # Outer training set (N-1 studies)\n",
    "    X_train_outer = X[train_mask]\n",
    "    y_train_outer = y[train_mask]\n",
    "    study_labels_outer = study_labels[train_mask] # Labels for outer training set\n",
    "\n",
    "    # Get the unique studies present in the outer training set\n",
    "    train_studies = np.unique(study_labels_outer)\n",
    "    print(f\"Outer training set contains studies: {train_studies.tolist()}\")\n",
    "\n",
    "    outer_fold_tasks = []\n",
    "\n",
    "    # Inner Loop: Iterate through each study in the outer training set to be used as VALIDATION set\n",
    "    for validation_study_name in train_studies:\n",
    "        print(f\"  Inner Loop: Validating on Study '{validation_study_name}'\")\n",
    "        # Create masks for inner split (relative to outer training data)\n",
    "        val_inner_mask = (study_labels_outer == validation_study_name)\n",
    "        train_inner_mask = ~val_inner_mask\n",
    "\n",
    "        # Inner training set (N-2 studies)\n",
    "        X_train_inner = X_train_outer[train_inner_mask]\n",
    "        y_train_inner = y_train_outer[train_inner_mask]\n",
    "        study_labels_inner = study_labels_outer[train_inner_mask] # Labels for inner training\n",
    "\n",
    "        # Inner validation set (1 study)\n",
    "        X_val_inner = X_train_outer[val_inner_mask]\n",
    "        y_val_inner = y_train_outer[val_inner_mask]\n",
    "\n",
    "\n",
    "        # --- Pre-process Data ONCE for this inner fold ---\n",
    "        # This computes processed versions for all n_genes values\n",
    "        processed_X_inner, y_train_inner_proc, y_val_inner_proc = pre_process_data_study_based(\n",
    "            n_genes_list,\n",
    "            X_train_inner, X_val_inner,\n",
    "            y_train_inner, y_val_inner,\n",
    "            study_labels_inner, # Pass inner training labels for pipeline fitting\n",
    "            pipe\n",
    "        )\n",
    "\n",
    "\n",
    "        # --- Create tasks for hyperparameter evaluation for THIS inner fold ---\n",
    "        for params in param_combos:\n",
    "            # Append a delayed evaluation task for each hyperparameter combination\n",
    "            outer_fold_tasks.append(delayed(evaluate_inner_fold)(\n",
    "                test_study_name,        # Identifier for the outer fold (held-out test study)\n",
    "                validation_study_name,  # Identifier for the inner fold (validation study)\n",
    "                processed_X_inner,      # Pre-calculated processed data for all n_genes\n",
    "                y_train_inner_proc,     # Inner training labels\n",
    "                y_val_inner_proc,       # Inner validation labels\n",
    "                model,                  # Classifier class\n",
    "                params,                 # Current hyperparameter combination\n",
    "                type = \"standard\"         # Choose evaluation type: \"standard\", \"OvR\", \"OvO\"\n",
    "            ))\n",
    "        # --- End Hyperparameter Loop ---\n",
    "    # --- End Inner Loop ---\n",
    "\n",
    "    # --- Execute tasks for the current outer fold in parallel ---\n",
    "    if outer_fold_tasks:\n",
    "        inner_results_list = Parallel(n_jobs=n_jobs, verbose=1)(outer_fold_tasks)\n",
    "\n",
    "        # Flatten the results if needed (depends on eval_type)\n",
    "        for res_item in inner_results_list:\n",
    "            if isinstance(res_item, list): # OvR or OvO might return lists\n",
    "                all_results.extend(res_item)\n",
    "            elif isinstance(res_item, dict): # Standard eval returns dict\n",
    "                all_results.append(res_item)\n",
    "            else:\n",
    "                 print(f\"Warning: Unexpected result type encountered: {type(res_item)}\")\n",
    "        print(f\"  Finished evaluations for outer fold '{test_study_name}'.\")\n",
    "    else:\n",
    "        print(f\"  No evaluation tasks generated for outer fold '{test_study_name}'.\")\n",
    "\n",
    "df_parallel_results_study_as_fold = pd.DataFrame(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.6951718400876972)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_cv_results(df, param_grid, label_mapping, score_col):\n",
    "    #  Extract param names and expand 'params'\n",
    "    param_names = list(param_grid.keys())\n",
    "    params_df = df['params'].apply(pd.Series)\n",
    "\n",
    "    # Normalize None values for groupby\n",
    "    for col in param_names:\n",
    "        if col in params_df.columns:\n",
    "            params_df[col] = params_df[col].apply(lambda x: 'none' if x is None else x)\n",
    "\n",
    "    #Combine expanded params with original DataFrame\n",
    "    df_with_params = pd.concat([df.drop(columns=['params']), params_df], axis=1)\n",
    "\n",
    "    # Determine group-by strategy based on evaluation type\n",
    "    if 'class' in df_with_params.columns:\n",
    "        # OvR\n",
    "        group_cols = param_names + ['class']\n",
    "        summary = df_with_params.groupby(group_cols)[score_col].mean().reset_index()\n",
    "        best = summary.loc[summary.groupby('class')[score_col].idxmax()].reset_index(drop=True)\n",
    "\n",
    "    elif 'class_0' in df_with_params.columns and 'class_1' in df_with_params.columns:\n",
    "        # OvO\n",
    "        group_cols = param_names + ['class_0', 'class_1']\n",
    "        summary = df_with_params.groupby(group_cols)[score_col].mean().reset_index()\n",
    "        best = summary.loc[summary.groupby(['class_0', 'class_1'])[score_col].idxmax()].reset_index(drop=True)\n",
    "\n",
    "    else:\n",
    "        # Standard multiclass\n",
    "        group_cols = param_names\n",
    "        summary = df_with_params.groupby(group_cols)[score_col].mean().reset_index()\n",
    "        best = summary.loc[[summary[score_col].idxmax()]].reset_index(drop=True)\n",
    "\n",
    "    int_to_label = {v: k for k, v in label_mapping.items()}\n",
    "    if 'class' in best.columns:\n",
    "        # OvR case\n",
    "        best['class_label'] = best['class'].map(int_to_label)\n",
    "        return best\n",
    "\n",
    "    elif 'class_0' in best.columns and 'class_1' in best.columns:\n",
    "        # OvO case\n",
    "        best['class_0_label'] = best['class_0'].map(int_to_label)\n",
    "        best['class_1_label'] = best['class_1'].map(int_to_label)\n",
    "        return best\n",
    "\n",
    "    else:\n",
    "        return best\n",
    "\n",
    "best_per_class_df = process_cv_results(\n",
    "    df_parallel_results_study_as_fold,\n",
    "    param_grid=param_grid,\n",
    "    label_mapping = label_mapping,\n",
    "    score_col = \"kappa\"\n",
    ")\n",
    "best_per_class_df[\"kappa\"].mean() # np.float64(0.5714442426161126)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   n_genes  class_weight  max_depth     kappa\n",
      "0     1000          True          5  0.717737\n",
      "   n_genes  class_weight  max_depth     kappa\n",
      "0     1000          True          2  0.715102\n"
     ]
    }
   ],
   "source": [
    "for outer_fold in (0,1):\n",
    "    df2 = df_parallel_results[df_parallel_results['outer_fold'] == outer_fold]\n",
    "    print(process_cv_results(\n",
    "        df2,\n",
    "        param_grid=param_grid,\n",
    "        label_mapping = label_mapping,\n",
    "        score_col = \"kappa\"\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'outer_fold': 0, 'mcc votes': np.float64(0.8450958861113496), 'kappa votes': np.float64(0.8447055763390022)}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     26\u001b[39m         processed_X[n_genes_i] = [X_train_n_genes, X_test_n_genes]\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m processed_X\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m processed_X = \u001b[43mpre_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_per_class_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m results_per_class = []\n\u001b[32m     32\u001b[39m df_outer_fold_results = df_parallel_results[df_parallel_results[\u001b[33m'\u001b[39m\u001b[33mouter_fold\u001b[39m\u001b[33m'\u001b[39m] == outer_fold].copy()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mpre_process_data\u001b[39m\u001b[34m(df, pipe)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m n_genes_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(df[\u001b[33m\"\u001b[39m\u001b[33mn_genes\u001b[39m\u001b[33m\"\u001b[39m])):\n\u001b[32m     21\u001b[39m     pipe_outer = clone(pipe)    \n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     X_train_n_genes = \u001b[43mpipe_outer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_outer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m                                            \u001b[49m\u001b[43mfeature_selection__study_per_patient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstudy_labels_outer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m                                            \u001b[49m\u001b[43mfeature_selection__n_genes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_genes_i\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m     X_test_n_genes = pipe_outer.transform(X_test_outer)\n\u001b[32m     26\u001b[39m     processed_X[n_genes_i] = [X_train_n_genes, X_test_n_genes]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Leukem.ai/.venv/lib/python3.12/site-packages/sklearn/base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Leukem.ai/.venv/lib/python3.12/site-packages/sklearn/pipeline.py:718\u001b[39m, in \u001b[36mPipeline.fit_transform\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    679\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Fit the model and transform with the final estimator.\u001b[39;00m\n\u001b[32m    680\u001b[39m \n\u001b[32m    681\u001b[39m \u001b[33;03mFit all the transformers one after the other and sequentially transform\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    715\u001b[39m \u001b[33;03m    Transformed samples.\u001b[39;00m\n\u001b[32m    716\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    717\u001b[39m routed_params = \u001b[38;5;28mself\u001b[39m._check_method_params(method=\u001b[33m\"\u001b[39m\u001b[33mfit_transform\u001b[39m\u001b[33m\"\u001b[39m, props=params)\n\u001b[32m--> \u001b[39m\u001b[32m718\u001b[39m Xt = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m last_step = \u001b[38;5;28mself\u001b[39m._final_estimator\n\u001b[32m    721\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[33m\"\u001b[39m\u001b[33mPipeline\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m._log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.steps) - \u001b[32m1\u001b[39m)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Leukem.ai/.venv/lib/python3.12/site-packages/sklearn/pipeline.py:588\u001b[39m, in \u001b[36mPipeline._fit\u001b[39m\u001b[34m(self, X, y, routed_params, raw_params)\u001b[39m\n\u001b[32m    581\u001b[39m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[32m    582\u001b[39m step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    583\u001b[39m     step_idx=step_idx,\n\u001b[32m    584\u001b[39m     step_params=routed_params[name],\n\u001b[32m    585\u001b[39m     all_params=raw_params,\n\u001b[32m    586\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m588\u001b[39m X, fitted_transformer = \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    589\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    590\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    592\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    593\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPipeline\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    594\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    595\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstep_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    596\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    597\u001b[39m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[32m    598\u001b[39m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[32m    599\u001b[39m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[32m    600\u001b[39m \u001b[38;5;28mself\u001b[39m.steps[step_idx] = (name, fitted_transformer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Leukem.ai/.venv/lib/python3.12/site-packages/joblib/memory.py:312\u001b[39m, in \u001b[36mNotMemorizedFunc.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m312\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Leukem.ai/.venv/lib/python3.12/site-packages/sklearn/pipeline.py:1551\u001b[39m, in \u001b[36m_fit_transform_one\u001b[39m\u001b[34m(transformer, X, y, weight, message_clsname, message, params)\u001b[39m\n\u001b[32m   1549\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[32m   1550\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[33m\"\u001b[39m\u001b[33mfit_transform\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1551\u001b[39m         res = \u001b[43mtransformer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit_transform\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1552\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1553\u001b[39m         res = transformer.fit(X, y, **params.get(\u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m, {})).transform(\n\u001b[32m   1554\u001b[39m             X, **params.get(\u001b[33m\"\u001b[39m\u001b[33mtransform\u001b[39m\u001b[33m\"\u001b[39m, {})\n\u001b[32m   1555\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Leukem.ai/.venv/lib/python3.12/site-packages/sklearn/utils/_set_output.py:319\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    321\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    322\u001b[39m         return_tuple = (\n\u001b[32m    323\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    324\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    325\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Leukem.ai/.venv/lib/python3.12/site-packages/sklearn/base.py:918\u001b[39m, in \u001b[36mTransformerMixin.fit_transform\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    903\u001b[39m         warnings.warn(\n\u001b[32m    904\u001b[39m             (\n\u001b[32m    905\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) has a `transform`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[32m    914\u001b[39m         )\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    917\u001b[39m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m.transform(X)\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    920\u001b[39m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fit(X, y, **fit_params).transform(X)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Leukem.ai/python_predict/transformers.py:145\u001b[39m, in \u001b[36mFeatureSelection.fit\u001b[39m\u001b[34m(self, X, y, study_per_patient, n_genes)\u001b[39m\n\u001b[32m    143\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    144\u001b[39m     X_study_arr = X[mask, :]\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m     top_genes_by_study[study] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compute_top_genes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_study_arr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[38;5;66;03m# Compute the intersection of top genes and preserve the original order.\u001b[39;00m\n\u001b[32m    148\u001b[39m intersect_genes = \u001b[38;5;28mset\u001b[39m.intersection(*top_genes_by_study.values())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Leukem.ai/python_predict/transformers.py:116\u001b[39m, in \u001b[36mFeatureSelection._compute_top_genes\u001b[39m\u001b[34m(self, X_arr)\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_compute_top_genes\u001b[39m(\u001b[38;5;28mself\u001b[39m, X_arr):\n\u001b[32m    115\u001b[39m     \u001b[38;5;66;03m# Compute the median for each gene.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     medians = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmedian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_arr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    117\u001b[39m     \u001b[38;5;66;03m# Compute the MAD: median of absolute deviations.\u001b[39;00m\n\u001b[32m    118\u001b[39m     mad = np.median(np.abs(X_arr - medians), axis=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Leukem.ai/.venv/lib/python3.12/site-packages/numpy/lib/_function_base_impl.py:3876\u001b[39m, in \u001b[36mmedian\u001b[39m\u001b[34m(a, axis, out, overwrite_input, keepdims)\u001b[39m\n\u001b[32m   3786\u001b[39m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_median_dispatcher)\n\u001b[32m   3787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmedian\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, overwrite_input=\u001b[38;5;28;01mFalse\u001b[39;00m, keepdims=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   3788\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3789\u001b[39m \u001b[33;03m    Compute the median along the specified axis.\u001b[39;00m\n\u001b[32m   3790\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3874\u001b[39m \n\u001b[32m   3875\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3876\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ureduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_median\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3877\u001b[39m \u001b[43m                    \u001b[49m\u001b[43moverwrite_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverwrite_input\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Leukem.ai/.venv/lib/python3.12/site-packages/numpy/lib/_function_base_impl.py:3764\u001b[39m, in \u001b[36m_ureduce\u001b[39m\u001b[34m(a, func, keepdims, **kwargs)\u001b[39m\n\u001b[32m   3761\u001b[39m             index_out = (\u001b[32m0\u001b[39m, ) * nd\n\u001b[32m   3762\u001b[39m             kwargs[\u001b[33m'\u001b[39m\u001b[33mout\u001b[39m\u001b[33m'\u001b[39m] = out[(\u001b[38;5;28mEllipsis\u001b[39m, ) + index_out]\n\u001b[32m-> \u001b[39m\u001b[32m3764\u001b[39m r = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3766\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3767\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Leukem.ai/.venv/lib/python3.12/site-packages/numpy/lib/_function_base_impl.py:3909\u001b[39m, in \u001b[36m_median\u001b[39m\u001b[34m(a, axis, out, overwrite_input)\u001b[39m\n\u001b[32m   3907\u001b[39m         part = a\n\u001b[32m   3908\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3909\u001b[39m     part = \u001b[43mpartition\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3911\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m part.shape == ():\n\u001b[32m   3912\u001b[39m     \u001b[38;5;66;03m# make 0-D arrays work\u001b[39;00m\n\u001b[32m   3913\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m part.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Leukem.ai/.venv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:819\u001b[39m, in \u001b[36mpartition\u001b[39m\u001b[34m(a, kth, axis, kind, order)\u001b[39m\n\u001b[32m    817\u001b[39m     axis = -\u001b[32m1\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m819\u001b[39m     a = \u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mK\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    820\u001b[39m a.partition(kth, axis=axis, kind=kind, order=order)\n\u001b[32m    821\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "type_eval = \"OvO\"\n",
    "columns_to_drop = [\"f1_macro\",\"f1_binary\", \"class_label\", \"class_0_label\", \"class_1_label\", \"kappa\", \"mcc\", \"accuracy\"]\n",
    "columns_to_drop_existing = [col for col in columns_to_drop if col in best_per_class_df.columns]\n",
    "\n",
    "per_class_results = []\n",
    "overall_results = []\n",
    "\n",
    "int_to_label = {v: k for k, v in label_mapping.items()}\n",
    "\n",
    "for outer_fold, (train_idx, test_idx) in enumerate(outer_cv.split(X, y)):\n",
    "\n",
    "    X_train_outer, X_test_outer = X[train_idx], X[test_idx]\n",
    "    y_train_outer, y_test_outer = y[train_idx], y[test_idx]\n",
    "    study_labels_outer = study_labels[train_idx]\n",
    "\n",
    "    classes_in_fold = np.unique(y_train_outer)\n",
    "    \n",
    "    def pre_process_data(df, pipe):\n",
    "        processed_X = {}\n",
    "        for n_genes_i in list(set(df[\"n_genes\"])):\n",
    "            pipe_outer = clone(pipe)    \n",
    "            X_train_n_genes = pipe_outer.fit_transform(X_train_outer, \n",
    "                                                    feature_selection__study_per_patient=study_labels_outer, \n",
    "                                                    feature_selection__n_genes=n_genes_i)\n",
    "            X_test_n_genes = pipe_outer.transform(X_test_outer)\n",
    "            processed_X[n_genes_i] = [X_train_n_genes, X_test_n_genes]\n",
    "        return processed_X\n",
    "    \n",
    "    processed_X = pre_process_data(best_per_class_df, pipe)\n",
    "    results_per_class = []\n",
    "\n",
    "    df_outer_fold_results = df_parallel_results[df_parallel_results['outer_fold'] == outer_fold].copy()\n",
    "\n",
    "    # Find the best parameters based ONLY on this outer fold's inner results\n",
    "    best_params_for_outer_fold = process_cv_results(\n",
    "        df_outer_fold_results,\n",
    "        param_grid=param_grid,\n",
    "        label_mapping=label_mapping,\n",
    "        score_col='kappa' \n",
    "    )\n",
    "\n",
    "    def standard_eval():\n",
    "        df = best_params_for_outer_fold\n",
    "        df = df.drop(columns=columns_to_drop_existing)\n",
    "        params = params.iloc[0].to_dict()\n",
    "\n",
    "        n_genes = params.pop('n_genes')\n",
    "        X_train_n_genes = processed_X[n_genes][0]\n",
    "        X_test_n_genes = processed_X[n_genes][1]\n",
    "\n",
    "        y_train_outer = np.array(y_train_outer, dtype=np.int32)\n",
    "        y_test_outer = np.array(y_test_outer, dtype=np.int32)\n",
    "\n",
    "        clf = clone(model(**params))\n",
    "        clf.fit(X_train_n_genes, y_train_outer)\n",
    "        preds = clf.predict(X_test_n_genes)\n",
    "        #preds = preds[:, 1]\n",
    "        #preds = (preds >= 0.5).astype(int)\n",
    "        results_per_class = {}\n",
    "        results_overall = {\n",
    "            'outer_fold': outer_fold,\n",
    "            'accuracy': accuracy_score(y_test_outer, preds),\n",
    "            'f1_macro': f1_score(y_test_outer, preds, average='macro'),\n",
    "            'mcc': matthews_corrcoef(y_test_outer, preds),\n",
    "            'kappa': cohen_kappa_score(y_test_outer, preds)\n",
    "        }\n",
    "        return results_overall, results_per_class\n",
    "\n",
    "    def ovr_eval():\n",
    "        df = best_params_for_outer_fold\n",
    "        df = df.drop(columns=columns_to_drop_existing)\n",
    "        df = df[df['class'].isin(classes_in_fold)]\n",
    "\n",
    "        prob_df = pd.DataFrame(index = test_idx, columns = classes_in_fold)\n",
    "\n",
    "        for cl in classes_in_fold:        \n",
    "            params = df[df[\"class\"] == cl]\n",
    "            params = params.iloc[0].to_dict()\n",
    "\n",
    "            n_genes = params.pop('n_genes')\n",
    "            params.pop('class', None)\n",
    "\n",
    "            X_train_n_genes = processed_X[n_genes][0]\n",
    "            X_test_n_genes = processed_X[n_genes][1]\n",
    "\n",
    "            if params[\"class_weight\"]=='none':\n",
    "                params.pop('class_weight')\n",
    "                \n",
    "            clf = clone(model(**params))\n",
    "            params['n_genes'] = n_genes\n",
    "\n",
    "            y_train_bin = [1 if yy == cl else 0 for yy in y_train_outer]\n",
    "            y_test_bin = [1 if yy == cl else 0 for yy in y_test_outer]\n",
    "\n",
    "            y_train_bin = np.array(y_train_bin, dtype=np.int32)\n",
    "            y_test_bin = np.array(y_test_bin, dtype=np.int32)\n",
    "\n",
    "            clf.fit(X_train_n_genes, y_train_bin)\n",
    "\n",
    "            preds_proba = clf.predict_proba(X_test_n_genes)[:, 1]\n",
    "            preds = (preds_proba >= 0.5).astype(int)\n",
    "            results_per_class.append({\n",
    "                'outer_fold': outer_fold,\n",
    "                'class': cl,\n",
    "                'accuracy': accuracy_score(y_test_bin, preds),\n",
    "                'f1_binary': f1_score(y_test_bin, preds, average='binary', pos_label=1),\n",
    "                'mcc': matthews_corrcoef(y_test_bin, preds),\n",
    "                'kappa': cohen_kappa_score(y_test_bin, preds)\n",
    "            })\n",
    "            prob_df[cl] = preds_proba\n",
    "\n",
    "        results_overall = {\n",
    "            'outer_fold': outer_fold,\n",
    "            'accuracy': accuracy_score(y_test_outer, prob_df.idxmax(axis=1)),\n",
    "            'f1_macro': f1_score(y_test_outer, prob_df.idxmax(axis=1), average='macro'),\n",
    "            'mcc': matthews_corrcoef(y_test_outer, prob_df.idxmax(axis=1)),\n",
    "            'kappa': cohen_kappa_score(y_test_outer, prob_df.idxmax(axis=1))\n",
    "        }\n",
    "        return results_overall, results_per_class\n",
    "\n",
    "    def ovo_eval():\n",
    "        df = best_params_for_outer_fold\n",
    "        df = df.drop(columns=columns_to_drop_existing)\n",
    "        df = df[(df['class_0'].isin(classes_in_fold)) & (df['class_1'].isin(classes_in_fold))]\n",
    "\n",
    "        results_per_class = []\n",
    "        \n",
    "        # Voting\n",
    "        prob_df = pd.DataFrame(0, index = test_idx, columns = classes_in_fold)\n",
    "        \n",
    "        for index, df_row in df.iterrows():\n",
    "            i = df_row[\"class_0\"]\n",
    "            j = df_row[\"class_1\"]\n",
    "\n",
    "            params = df_row.to_dict()\n",
    "\n",
    "            n_genes = params.pop('n_genes')\n",
    "            params.pop('class_0', None)\n",
    "            params.pop('class_1', None)\n",
    "\n",
    "            X_train_n_genes, X_test_n_genes = processed_X[n_genes]\n",
    "\n",
    "            train_mask = [(yy == i or yy == j) for yy in y_train_outer]\n",
    "            val_mask = [(yy == i or yy == j) for yy in y_test_outer]\n",
    "\n",
    "            X_train_ij = X_train_n_genes[train_mask]\n",
    "            y_train_ij = np.array([yy for yy in y_train_outer if yy == i or yy == j], dtype=np.int32) \n",
    "            y_train_ij = (y_train_ij == i).astype(np.int32) # label '1' for class i, and '0' for class j\n",
    "\n",
    "            X_val_ij = X_test_n_genes[val_mask]\n",
    "            y_val_ij = np.array([yy for yy in y_test_outer if yy == i or yy == j], dtype=np.int32) \n",
    "            y_val_ij = (y_val_ij == i).astype(np.int32) # label '1' for class i, and '0' for class j\n",
    "            \n",
    "            if params[\"class_weight\"]=='none':\n",
    "                params.pop('class_weight')\n",
    "\n",
    "            clf = clone(model(**params))\n",
    "            \n",
    "            clf.fit(X_train_ij, y_train_ij)\n",
    "            \n",
    "            preds = clf.predict_proba(X_val_ij)\n",
    "            preds = preds[:, 1]\n",
    "            preds = (preds >= 0.5).astype(int)\n",
    "\n",
    "            results_per_class.append({\n",
    "                'outer_fold': outer_fold,\n",
    "                'class_0': i,\n",
    "                'class_1': j,\n",
    "                'f1_binary': f1_score(y_val_ij, preds, average='binary', pos_label=1),\n",
    "                'mcc': matthews_corrcoef(y_val_ij, preds),\n",
    "                'kappa': cohen_kappa_score(y_val_ij, preds)\n",
    "            })\n",
    "\n",
    "            preds_all_test_orig = clf.predict_proba(X_test_n_genes)\n",
    "            preds_all_test_orig = preds_all_test_orig[:, 1]\n",
    "            preds_all_test = (preds_all_test_orig >= 0.5).astype(int)\n",
    "\n",
    "            for prediction_idx, prediction in enumerate(preds_all_test):\n",
    "                if prediction == 1:\n",
    "                    prob_df.loc[test_idx[prediction_idx], i] += 1\n",
    "                else:\n",
    "                    prob_df.loc[test_idx[prediction_idx], j] += 1\n",
    "\n",
    "        # votes\n",
    "        preds_all = prob_df.idxmax(axis=1)\n",
    "\n",
    "        results_overall = {\n",
    "            'outer_fold': outer_fold,\n",
    "            #'accuracy': accuracy_score(y_test_outer, preds_all),\n",
    "            #'f1_macro': f1_score(y_test_outer, preds_all, average='macro'),\n",
    "            'mcc votes': matthews_corrcoef(y_test_outer, preds_all),\n",
    "            'kappa votes': cohen_kappa_score(y_test_outer, preds_all)\n",
    "        }\n",
    "        return results_overall, results_per_class\n",
    "\n",
    "    # Dispatch table for clean logic\n",
    "    eval_dispatch = {\n",
    "        'OvR': ovr_eval,\n",
    "        'OvO': ovo_eval\n",
    "    }\n",
    "\n",
    "    if type_eval not in eval_dispatch:\n",
    "        raise ValueError(f\"Unsupported evaluation type: {type}\")\n",
    "    \n",
    "    results_overall, results_per_class = eval_dispatch[type_eval]()\n",
    "    per_class_results.append(results_per_class)\n",
    "    print(results_overall)\n",
    "    overall_results.append(results_overall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OvO: {'outer_fold': 0, 'mcc votes': np.float64(0.8450958861113496), 'kappa votes': np.float64(0.8447055763390022)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'outer_fold': 0,\n",
       "  'class_0': 0,\n",
       "  'class_1': 1,\n",
       "  'f1_binary': 0.9158878504672897,\n",
       "  'mcc': np.float64(0.8066855435212543),\n",
       "  'kappa': np.float64(0.8049792531120332)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 0,\n",
       "  'class_1': 2,\n",
       "  'f1_binary': 0.9636363636363636,\n",
       "  'mcc': np.float64(0.7414141414141414),\n",
       "  'kappa': np.float64(0.7414141414141414)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 0,\n",
       "  'class_1': 3,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 0,\n",
       "  'class_1': 4,\n",
       "  'f1_binary': 0.9818181818181818,\n",
       "  'mcc': np.float64(0.7318181818181818),\n",
       "  'kappa': np.float64(0.7318181818181818)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 0,\n",
       "  'class_1': 5,\n",
       "  'f1_binary': 0.9557522123893806,\n",
       "  'mcc': np.float64(0.9163448298943221),\n",
       "  'kappa': np.float64(0.915156744319816)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 0,\n",
       "  'class_1': 6,\n",
       "  'f1_binary': 0.9345794392523364,\n",
       "  'mcc': np.float64(0.7504283707412239),\n",
       "  'kappa': np.float64(0.7459677419354839)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 0,\n",
       "  'class_1': 7,\n",
       "  'f1_binary': 0.990990990990991,\n",
       "  'mcc': np.float64(0.917516612761795),\n",
       "  'kappa': np.float64(0.9141274238227147)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 0,\n",
       "  'class_1': 8,\n",
       "  'f1_binary': 0.972972972972973,\n",
       "  'mcc': np.float64(0.943443729531313),\n",
       "  'kappa': np.float64(0.9432750624331074)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 0,\n",
       "  'class_1': 9,\n",
       "  'f1_binary': 0.9345794392523364,\n",
       "  'mcc': np.float64(0.8225225129453326),\n",
       "  'kappa': np.float64(0.8200734394124847)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 0,\n",
       "  'class_1': 10,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 0,\n",
       "  'class_1': 11,\n",
       "  'f1_binary': 0.9734513274336283,\n",
       "  'mcc': np.float64(0.7698522025642995),\n",
       "  'kappa': np.float64(0.7442489851150202)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 0,\n",
       "  'class_1': 12,\n",
       "  'f1_binary': 0.9734513274336283,\n",
       "  'mcc': np.float64(0.6158817620514396),\n",
       "  'kappa': np.float64(0.5499999999999999)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 0,\n",
       "  'class_1': 13,\n",
       "  'f1_binary': 0.9532710280373832,\n",
       "  'mcc': np.float64(0.7624425757515653),\n",
       "  'kappa': np.float64(0.7540983606557377)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 0,\n",
       "  'class_1': 14,\n",
       "  'f1_binary': 0.9908256880733946,\n",
       "  'mcc': np.float64(0.917364944587726),\n",
       "  'kappa': np.float64(0.9139633286318759)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 0,\n",
       "  'class_1': 15,\n",
       "  'f1_binary': 0.9821428571428571,\n",
       "  'mcc': np.float64(0.5671308728156005),\n",
       "  'kappa': np.float64(0.48672566371681414)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 0,\n",
       "  'class_1': 16,\n",
       "  'f1_binary': 0.9649122807017544,\n",
       "  'mcc': np.float64(0.7478783550139054),\n",
       "  'kappa': np.float64(0.717391304347826)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 1,\n",
       "  'class_1': 2,\n",
       "  'f1_binary': 0.9620253164556962,\n",
       "  'mcc': np.float64(0.7877263614433762),\n",
       "  'kappa': np.float64(0.7857142857142857)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 1,\n",
       "  'class_1': 3,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 1,\n",
       "  'class_1': 4,\n",
       "  'f1_binary': 0.975,\n",
       "  'mcc': np.float64(0.6896446592974972),\n",
       "  'kappa': np.float64(0.6446280991735538)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 1,\n",
       "  'class_1': 5,\n",
       "  'f1_binary': 0.975,\n",
       "  'mcc': np.float64(0.959698946213619),\n",
       "  'kappa': np.float64(0.9588875453446191)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 1,\n",
       "  'class_1': 6,\n",
       "  'f1_binary': 0.975,\n",
       "  'mcc': np.float64(0.9161393640106364),\n",
       "  'kappa': np.float64(0.9126365054602184)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 1,\n",
       "  'class_1': 7,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 1,\n",
       "  'class_1': 8,\n",
       "  'f1_binary': 0.9873417721518988,\n",
       "  'mcc': np.float64(0.9776923610938035),\n",
       "  'kappa': np.float64(0.9774436090225564)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 1,\n",
       "  'class_1': 9,\n",
       "  'f1_binary': 0.961038961038961,\n",
       "  'mcc': np.float64(0.9106192160459141),\n",
       "  'kappa': np.float64(0.9102112676056338)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 1,\n",
       "  'class_1': 10,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 1,\n",
       "  'class_1': 11,\n",
       "  'f1_binary': 0.9873417721518988,\n",
       "  'mcc': np.float64(0.9236476600955582),\n",
       "  'kappa': np.float64(0.9207419898819562)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 1,\n",
       "  'class_1': 12,\n",
       "  'f1_binary': 0.9620253164556962,\n",
       "  'mcc': np.float64(0.6340751391209737),\n",
       "  'kappa': np.float64(0.6292134831460674)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 1,\n",
       "  'class_1': 13,\n",
       "  'f1_binary': 0.987012987012987,\n",
       "  'mcc': np.float64(0.9450726919990827),\n",
       "  'kappa': np.float64(0.9435665914221218)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 1,\n",
       "  'class_1': 14,\n",
       "  'f1_binary': 0.987012987012987,\n",
       "  'mcc': np.float64(0.9138735334633754),\n",
       "  'kappa': np.float64(0.9101796407185628)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 1,\n",
       "  'class_1': 15,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 1,\n",
       "  'class_1': 16,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 2,\n",
       "  'class_1': 3,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 2,\n",
       "  'class_1': 4,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 2,\n",
       "  'class_1': 5,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 2,\n",
       "  'class_1': 6,\n",
       "  'f1_binary': 0.9411764705882353,\n",
       "  'mcc': np.float64(0.9162456945817024),\n",
       "  'kappa': np.float64(0.912751677852349)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 2,\n",
       "  'class_1': 7,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 2,\n",
       "  'class_1': 8,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 2,\n",
       "  'class_1': 9,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 2,\n",
       "  'class_1': 10,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 2,\n",
       "  'class_1': 11,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 2,\n",
       "  'class_1': 12,\n",
       "  'f1_binary': 0.9473684210526315,\n",
       "  'mcc': np.float64(0.848528137423857),\n",
       "  'kappa': np.float64(0.8372093023255814)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 2,\n",
       "  'class_1': 13,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 2,\n",
       "  'class_1': 14,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 2,\n",
       "  'class_1': 15,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 2,\n",
       "  'class_1': 16,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 3,\n",
       "  'class_1': 4,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 3,\n",
       "  'class_1': 5,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 3,\n",
       "  'class_1': 6,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 3,\n",
       "  'class_1': 7,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 3,\n",
       "  'class_1': 8,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 3,\n",
       "  'class_1': 9,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 3,\n",
       "  'class_1': 10,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 3,\n",
       "  'class_1': 11,\n",
       "  'f1_binary': 0.9906542056074766,\n",
       "  'mcc': np.float64(0.9267126287562376),\n",
       "  'kappa': np.float64(0.9240348692403487)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 3,\n",
       "  'class_1': 12,\n",
       "  'f1_binary': 0.9814814814814815,\n",
       "  'mcc': np.float64(0.7603826787755086),\n",
       "  'kappa': np.float64(0.7327188940092166)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 3,\n",
       "  'class_1': 13,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 3,\n",
       "  'class_1': 14,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 3,\n",
       "  'class_1': 15,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 3,\n",
       "  'class_1': 16,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 4,\n",
       "  'class_1': 5,\n",
       "  'f1_binary': 0.8888888888888888,\n",
       "  'mcc': np.float64(0.8873001675315898),\n",
       "  'kappa': np.float64(0.8809946714031972)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 4,\n",
       "  'class_1': 6,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 4,\n",
       "  'class_1': 7,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 4,\n",
       "  'class_1': 8,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 4,\n",
       "  'class_1': 9,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 4,\n",
       "  'class_1': 10,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 4,\n",
       "  'class_1': 11,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 4,\n",
       "  'class_1': 12,\n",
       "  'f1_binary': 0.8888888888888888,\n",
       "  'mcc': np.float64(0.8),\n",
       "  'kappa': np.float64(0.7804878048780488)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 4,\n",
       "  'class_1': 13,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 4,\n",
       "  'class_1': 14,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 4,\n",
       "  'class_1': 15,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 4,\n",
       "  'class_1': 16,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 5,\n",
       "  'class_1': 6,\n",
       "  'f1_binary': 0.9838709677419355,\n",
       "  'mcc': np.float64(0.9307698415351914),\n",
       "  'kappa': np.float64(0.928379588182632)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 5,\n",
       "  'class_1': 7,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 5,\n",
       "  'class_1': 8,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 5,\n",
       "  'class_1': 9,\n",
       "  'f1_binary': 0.992,\n",
       "  'mcc': np.float64(0.9753577555557507),\n",
       "  'kappa': np.float64(0.9750542299349241)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 5,\n",
       "  'class_1': 10,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 5,\n",
       "  'class_1': 11,\n",
       "  'f1_binary': 0.9841269841269841,\n",
       "  'mcc': np.float64(0.8591269841269841),\n",
       "  'kappa': np.float64(0.8591269841269842)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 5,\n",
       "  'class_1': 12,\n",
       "  'f1_binary': 0.9763779527559056,\n",
       "  'mcc': np.float64(0.6479515952918626),\n",
       "  'kappa': np.float64(0.6433566433566433)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 5,\n",
       "  'class_1': 13,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 5,\n",
       "  'class_1': 14,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 5,\n",
       "  'class_1': 15,\n",
       "  'f1_binary': 0.9838709677419355,\n",
       "  'mcc': np.float64(0.7622023228463561),\n",
       "  'kappa': np.float64(0.7349397590361446)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 5,\n",
       "  'class_1': 16,\n",
       "  'f1_binary': 0.9841269841269841,\n",
       "  'mcc': np.float64(0.8841269841269841),\n",
       "  'kappa': np.float64(0.8841269841269841)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 6,\n",
       "  'class_1': 7,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 6,\n",
       "  'class_1': 8,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 6,\n",
       "  'class_1': 9,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 6,\n",
       "  'class_1': 10,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 6,\n",
       "  'class_1': 11,\n",
       "  'f1_binary': 0.9714285714285714,\n",
       "  'mcc': np.float64(0.9090593428863095),\n",
       "  'kappa': np.float64(0.9049429657794676)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 6,\n",
       "  'class_1': 12,\n",
       "  'f1_binary': 0.9444444444444444,\n",
       "  'mcc': np.float64(0.7326950970650465),\n",
       "  'kappa': np.float64(0.6986301369863014)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 6,\n",
       "  'class_1': 13,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 6,\n",
       "  'class_1': 14,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 6,\n",
       "  'class_1': 15,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 6,\n",
       "  'class_1': 16,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 7,\n",
       "  'class_1': 8,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 7,\n",
       "  'class_1': 9,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 7,\n",
       "  'class_1': 10,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 7,\n",
       "  'class_1': 11,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 7,\n",
       "  'class_1': 12,\n",
       "  'f1_binary': 0.9333333333333333,\n",
       "  'mcc': np.float64(0.8366600265340756),\n",
       "  'kappa': np.float64(0.8235294117647058)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 7,\n",
       "  'class_1': 13,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 7,\n",
       "  'class_1': 14,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 7,\n",
       "  'class_1': 15,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 7,\n",
       "  'class_1': 16,\n",
       "  'f1_binary': 0.9230769230769231,\n",
       "  'mcc': np.float64(0.8827348295047495),\n",
       "  'kappa': np.float64(0.8759124087591241)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 8,\n",
       "  'class_1': 9,\n",
       "  'f1_binary': 0.9900990099009901,\n",
       "  'mcc': np.float64(0.973505222533836),\n",
       "  'kappa': np.float64(0.9731543624161074)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 8,\n",
       "  'class_1': 10,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 8,\n",
       "  'class_1': 11,\n",
       "  'f1_binary': 0.9902912621359223,\n",
       "  'mcc': np.float64(0.9263763149622711),\n",
       "  'kappa': np.float64(0.9236739974126779)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 8,\n",
       "  'class_1': 12,\n",
       "  'f1_binary': 0.9902912621359223,\n",
       "  'mcc': np.float64(0.8857851797221404),\n",
       "  'kappa': np.float64(0.8793103448275862)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 8,\n",
       "  'class_1': 13,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 8,\n",
       "  'class_1': 14,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 8,\n",
       "  'class_1': 15,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 8,\n",
       "  'class_1': 16,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 9,\n",
       "  'class_1': 10,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 9,\n",
       "  'class_1': 11,\n",
       "  'f1_binary': 0.9666666666666667,\n",
       "  'mcc': np.float64(0.8376233659741151),\n",
       "  'kappa': np.float64(0.8246445497630331)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 9,\n",
       "  'class_1': 12,\n",
       "  'f1_binary': 0.9508196721311475,\n",
       "  'mcc': np.float64(0.6020797289396148),\n",
       "  'kappa': np.float64(0.5321100917431193)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 9,\n",
       "  'class_1': 13,\n",
       "  'f1_binary': 0.9491525423728814,\n",
       "  'mcc': np.float64(0.8081352297940448),\n",
       "  'kappa': np.float64(0.8064516129032258)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 9,\n",
       "  'class_1': 14,\n",
       "  'f1_binary': 0.9830508474576272,\n",
       "  'mcc': np.float64(0.8975274678557507),\n",
       "  'kappa': np.float64(0.8923076923076924)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 9,\n",
       "  'class_1': 15,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 9,\n",
       "  'class_1': 16,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 10,\n",
       "  'class_1': 11,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 10,\n",
       "  'class_1': 12,\n",
       "  'f1_binary': 0.9629629629629629,\n",
       "  'mcc': np.float64(0.8618916073713346),\n",
       "  'kappa': np.float64(0.8524590163934427)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 10,\n",
       "  'class_1': 13,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 10,\n",
       "  'class_1': 14,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 10,\n",
       "  'class_1': 15,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 10,\n",
       "  'class_1': 16,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 11,\n",
       "  'class_1': 12,\n",
       "  'f1_binary': 0.8888888888888888,\n",
       "  'mcc': np.float64(0.6928203230275508),\n",
       "  'kappa': np.float64(0.6486486486486487)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 11,\n",
       "  'class_1': 13,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 11,\n",
       "  'class_1': 14,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 11,\n",
       "  'class_1': 15,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 11,\n",
       "  'class_1': 16,\n",
       "  'f1_binary': 0.9333333333333333,\n",
       "  'mcc': np.float64(0.8918825850158447),\n",
       "  'kappa': np.float64(0.8860759493670887)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 12,\n",
       "  'class_1': 13,\n",
       "  'f1_binary': 0.8,\n",
       "  'mcc': np.float64(0.7090909090909091),\n",
       "  'kappa': np.float64(0.7090909090909091)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 12,\n",
       "  'class_1': 14,\n",
       "  'f1_binary': 0.8888888888888888,\n",
       "  'mcc': np.float64(0.8280786712108251),\n",
       "  'kappa': np.float64(0.8135593220338984)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 12,\n",
       "  'class_1': 15,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 12,\n",
       "  'class_1': 16,\n",
       "  'f1_binary': 0.75,\n",
       "  'mcc': np.float64(0.7071067811865475),\n",
       "  'kappa': np.float64(0.6666666666666667)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 13,\n",
       "  'class_1': 14,\n",
       "  'f1_binary': 0.9565217391304348,\n",
       "  'mcc': np.float64(0.8740073734751262),\n",
       "  'kappa': np.float64(0.8661417322834646)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 13,\n",
       "  'class_1': 15,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 13,\n",
       "  'class_1': 16,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 14,\n",
       "  'class_1': 15,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 14,\n",
       "  'class_1': 16,\n",
       "  'f1_binary': 1.0,\n",
       "  'mcc': np.float64(1.0),\n",
       "  'kappa': np.float64(1.0)},\n",
       " {'outer_fold': 0,\n",
       "  'class_0': 15,\n",
       "  'class_1': 16,\n",
       "  'f1_binary': 0.75,\n",
       "  'mcc': np.float64(0.6928203230275508),\n",
       "  'kappa': np.float64(0.6486486486486487)}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_class_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(y)) * (len(np.unique(y))-1) / 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
